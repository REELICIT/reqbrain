{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6d5e8dc-4cab-4b90-869f-7ffb2dd8e31d",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com//REELICIT/reqbrain_rep_package/blob/ee75058cf6ca3ff6fdf3cd6eeb1566ba61e95d24/inferencing_all_trained_models/all_models_used_with_langchain/falcon_inferance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ] 
  },
  {
   "cell_type": "markdown",
   "id": "b685a65f-571c-4529-b175-582d1172c1e4",
   "metadata": {},
   "source": [
    "# ReqBrain (Falcon 7b base) Download & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49be96bd-de36-48cd-8ecd-4f6048a8b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "\n",
    "# if not installed, use !pip install <name of the API>\n",
    "# in case of error due to lacking API:\n",
    "# Python writes the name and command to install the lacking API in the very last lines of the error message.\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch import cuda, bfloat16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab0c5c3-cca0-481d-810a-eb1d2cb004f7",
   "metadata": {},
   "source": [
    "# Resources Checkup\n",
    "\n",
    "- **Important Note:** A GPU with minimum of 32GB GPU memory is required to load the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6acf0703-2c42-4ae4-acdf-3c69b80f97f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 1 Name: Tesla V100-SXM2-32GB\n",
      "GPU 1 Total Memory: 31.74 GB\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "\n",
    "    # Iterate over each GPU and print its name and memory information\n",
    "    for i in range(num_gpus):\n",
    "        gpu = torch.cuda.get_device_properties(i)\n",
    "        print(f\"GPU {i + 1} Name: {gpu.name}\")\n",
    "        print(f\"GPU {i + 1} Total Memory: {gpu.total_memory / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. A GPU with minimum of 32GB GPU memory is required to load the model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd6e5c26-e971-49b6-96e6-d7695c7becc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detecting GPU device to load model on it\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bed8c3-101e-480f-bcae-4f5cce282908",
   "metadata": {},
   "source": [
    "# Downloading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b304279-e44b-41e4-9e9b-15aafc1572d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03261e75d334137aae6c6476fec80e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_name = 'REELICIT/falcon-7b-ReqBrain'\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, use_fast = False)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3b6c48-c95c-451c-8240-536601b80836",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abcdb1fb-447a-4033-81e1-2ff0bde8487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the list of stopping criteria for the model\n",
    "\n",
    "stop_token_ids = [\n",
    "    tokenizer.convert_tokens_to_ids(x) for x in [['Human', ':'], ['###', 'Human', ':'], [\"***\"], [\"###\"], [\"###\", \"Assistant\", \":\"], [tokenizer.eos_token]]\n",
    "]\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "106fa892-abd9-49fa-9dd3-4a27457a9281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c50a943-e82d-441b-a3ff-5aaa443b06e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 14:25:07.448538: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-19 14:25:07.964676: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-19 14:25:07.964708: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-19 14:25:07.964737: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-19 14:25:07.987047: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-19 14:25:09.473691: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "pipe = transformers.pipeline(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    return_full_text = True, # Set it to True when combining with LangChain\n",
    "    task = 'text-generation',\n",
    "    device = device,\n",
    "    stopping_criteria = stopping_criteria,  \n",
    "    temperature = 0.1,\n",
    "    top_p = 0.15,  \n",
    "    top_k = 0,  \n",
    "    max_new_tokens = 200,  # the number of tokens the model must generate\n",
    "    repetition_penalty = 1.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e0512b-7e11-429a-9dfb-5f6f7b09dd40",
   "metadata": {},
   "source": [
    "# Inferance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0e36ecd-15b0-4a35-93c5-4f03d7e6caf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.15` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - The font size and type of text used on the screen shall be clearly visible and easily readable on the parameters to be accessed.\n",
      "2 - The color contrast between background and foreground texts/images shall meet ISO 29148 standards.***\n"
     ]
    }
   ],
   "source": [
    "prompt_1 = \"I want two usability requirements for bus ticket machines.\" \n",
    "\n",
    "result = pipe(f\"### Human: {prompt_1}.### Assistant: \")\n",
    "print(result[0]['generated_text'].split('### Assistant: ')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7abff8bc-4fc9-4650-8e11-40b29988e5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) The system shall use default values only for parameters not requiring user intervention.\n",
      "2) The system shall discard any new transaction that arrives with a zero balance.\n",
      "3) The system shall prompt the operator to change passwords if more than five consecutive failures occur during authentication attempts. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_2 = \"Write me three security requirements for an ATM software?\"\n",
    "\n",
    "result = pipe(f\"### Human: {prompt_2}.### Assistant: \")\n",
    "print(result[0]['generated_text'].split('### Assistant: ')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709cedf0-69e6-4ccf-95f8-2b89153f01fa",
   "metadata": {},
   "source": [
    "## Try it for your self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d66a69ea-bc6e-4ad8-8d53-74f006b686b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below lines to use a custom query\n",
    "\n",
    "# your_prompt = 'your prompt goes here!'\n",
    "# result = pipe(f\"### Human: {your_prompt}.### Assistant: \")\n",
    "# print(result[0]['generated_text'].split('### Assistant: ')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28403787-c503-49e9-b6f5-209e08e986fb",
   "metadata": {},
   "source": [
    "# Model Setup with Langchain for Chat-based RE Elicitation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f8de78e-4ee7-4bfe-9326-86fd73b7001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing langchain required APIs\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf456ecc-f836-4b60-9fc7-2ed87e9b8253",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = HuggingFacePipeline(pipeline = pipe)\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key = \"history\",\n",
    "    k = 5,     # Chat history memory, increase/decrease it depending on how far the model should remember!\n",
    "    return_only_outputs = True)\n",
    "\n",
    "chat_history = ConversationChain(\n",
    "    llm = chat_model,\n",
    "    memory = memory,\n",
    "    verbose = True)\n",
    "\n",
    "chat_history.prompt.template = \"\"\"\n",
    "You are a professional requirements engineer who helps users brainstorm more software requirements.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "### Human: {input}\n",
    "### Assistant: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcb7b90f-a431-412f-a812-6ee2e5cebc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_re_elicitor(chat_chain, query):\n",
    "    chat_chain.predict(input = query)\n",
    "    last_message = chat_chain.memory.chat_memory.messages[-1]\n",
    "    last_message_content = last_message.content.split('\\n\\n')[0].strip()\n",
    "    \n",
    "    # cleaning prtompt text\n",
    "    prompt_id = last_message_content.find('\\n### Human:')\n",
    "    if prompt_id != -1:\n",
    "        last_message_content = last_message_content[:prompt_id]\n",
    "\n",
    "    # cleaning model generated text\n",
    "    for stop_token in ['***', '###', '### Assistant:', '### Human:', 'Human:']:\n",
    "        last_message_content = last_message_content.removesuffix(stop_token)\n",
    "\n",
    "    return last_message_content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0cde6af-a0a1-492e-b5e0-dbad84524bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.15` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a professional requirements engineer who helps users brainstorm more software requirements.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "### Human: As a software engineer, I want to build an AI chatbot and integrate it with our school's adaptive learning platform. \n",
      "The chatbot should be capable of doing content search in the system, doing quizzes, motivating, real-time interaction with students, resolution of misunderstandings, and delivering curriculum content in various formats. \n",
      "It also needs the ability to provide topic definitions, connect with a recommendation system, and engage in small talk for a more interactive experience.\n",
      "I want you to brainstorm requirements for me.\n",
      "### Assistant: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "(1) Content Search In The System; (2) Quizzes; (3) Motivates; (4) Real-Time Interaction With Students; (5) Resolves Misunderstandings; (6) Delivers Curriculum Content In Various Formats; (7) Topic Definitions; (8) Connects With A Recommendation System; (9) Engages In Small Talk; And (10) Is An Adaptive Learning Platform.)\n",
      "(11) An adaptive learning platform is required. (12) An integration between the adaptive learning platform and our school's adaptive learning platform shall be provided.\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "llm_re_elicitor(chat_history, '''As a software engineer, I want to build an AI chatbot and integrate it with our school's adaptive learning platform. \n",
    "The chatbot should be capable of doing content search in the system, doing quizzes, motivating, real-time interaction with students, resolution of misunderstandings, and delivering curriculum content in various formats. \n",
    "It also needs the ability to provide topic definitions, connect with a recommendation system, and engage in small talk for a more interactive experience.\n",
    "I want you to brainstorm requirements for me.''')\n",
    "print(chat_history.memory.chat_memory.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13463585-ed8c-40fb-9568-624aa98c5024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.15` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a professional requirements engineer who helps users brainstorm more software requirements.\n",
      "\n",
      "Current conversation:\n",
      "Human: As a software engineer, I want to build an AI chatbot and integrate it with our school's adaptive learning platform. \n",
      "The chatbot should be capable of doing content search in the system, doing quizzes, motivating, real-time interaction with students, resolution of misunderstandings, and delivering curriculum content in various formats. \n",
      "It also needs the ability to provide topic definitions, connect with a recommendation system, and engage in small talk for a more interactive experience.\n",
      "I want you to brainstorm requirements for me.\n",
      "AI: (1) Content Search In The System; (2) Quizzes; (3) Motivates; (4) Real-Time Interaction With Students; (5) Resolves Misunderstandings; (6) Delivers Curriculum Content In Various Formats; (7) Topic Definitions; (8) Connects With A Recommendation System; (9) Engages In Small Talk; And (10) Is An Adaptive Learning Platform.)\n",
      "(11) An adaptive learning platform is required. (12) An integration between the adaptive learning platform and our school's adaptive learning platform shall be provided.\n",
      "###\n",
      "### Human: 'Concerning requirement number 3, which you proposed in your previous message, tell me two methods I can employ to motivate students?\n",
      "### Assistant: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1) Use motivational language such as \"enjoy this course\" or \"learn something new\".\n",
      "2) Provide examples of topics that students might find interesting.')\n",
      "\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "llm_re_elicitor(chat_history, ''''Concerning requirement number 3, which you proposed in your previous message, tell me two methods I can employ to motivate students?''')\n",
    "print(chat_history.memory.chat_memory.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdc09590-600f-41e4-bbe2-75b6a84d91d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your prtompt between ''' marks for continuation of the chat!\n",
    "\n",
    "# llm_re_elicitor(chat_history, '''' ''')\n",
    "# print(chat_history.memory.chat_memory.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0cdda4-6f56-4449-9c84-909327cf5689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
