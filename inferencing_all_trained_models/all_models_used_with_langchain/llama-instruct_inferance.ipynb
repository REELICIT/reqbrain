{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "044fe8b4-b91a-4fb4-86c6-532325b10d3c",
   "metadata": {},
   "source": [
    "<a href=\"\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647fae37-a9e7-4f68-8139-8edeb034bdab",
   "metadata": {},
   "source": [
    "# ReqBrain (Llama-2 7b) Download & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49be96bd-de36-48cd-8ecd-4f6048a8b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "\n",
    "# if not installed, use !pip install <name of the API>\n",
    "# in case of error due to lacking API:\n",
    "# Python writes the name and command to install the lacking API in the very last lines of the error message.\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch import cuda, bfloat16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99db03e2-6486-46d5-8ae3-298f0f9226c4",
   "metadata": {},
   "source": [
    "# Resources Checkup\n",
    "\n",
    "- **Important Note:** A GPU with minimum of 32GB GPU memory is required to load the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d155263-a87d-439b-8e35-b69c1e7fe4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 1 Name: Tesla V100-SXM2-32GB\n",
      "GPU 1 Total Memory: 31.74 GB\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "\n",
    "    # Iterate over each GPU and print its name and memory information\n",
    "    for i in range(num_gpus):\n",
    "        gpu = torch.cuda.get_device_properties(i)\n",
    "        print(f\"GPU {i + 1} Name: {gpu.name}\")\n",
    "        print(f\"GPU {i + 1} Total Memory: {gpu.total_memory / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. A GPU with minimum of 32GB GPU memory is required to load the model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "958b2803-7b6e-4620-8551-7983fceac619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detecting GPU device to load model on it\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261a726b-210f-47f5-996a-4c91d51a68cb",
   "metadata": {},
   "source": [
    "# Downloading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b304279-e44b-41e4-9e9b-15aafc1572d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a5b2c1552542d0ad405976fa2b41ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39191695ac9e4015802e228d1751e118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968afa2db11945a185cb509819581c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_name = 'REELICIT/Llama-2-7b-chat-hf-ReqBrain'\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, use_fast = False)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e22297-2e8a-458c-82db-7825ebc0146a",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abcdb1fb-447a-4033-81e1-2ff0bde8487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the list of stopping criteria for the model\n",
    "\n",
    "___inst = tokenizer.convert_ids_to_tokens(tokenizer(\"[/INST]\")[\"input_ids\"])[1:]\n",
    "___end_of_ = tokenizer.convert_ids_to_tokens(tokenizer(\"/end_of_\")[\"input_ids\"])[1:]\n",
    "___user = tokenizer.convert_ids_to_tokens(tokenizer(\"[/user]\")[\"input_ids\"])[1:]\n",
    "\n",
    "stop_token_ids = [\n",
    "    tokenizer.convert_tokens_to_ids(x) for x in [___inst, [tokenizer.eos_token], ___end_of_, ___user, ['```']]\n",
    "]\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "106fa892-abd9-49fa-9dd3-4a27457a9281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c50a943-e82d-441b-a3ff-5aaa443b06e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 11:29:54.081856: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-22 11:29:54.591808: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-22 11:29:54.591849: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-22 11:29:54.591880: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-22 11:29:54.618164: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-22 11:29:56.581377: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "pipe = transformers.pipeline(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    return_full_text = True, # Set it to True when combining with LangChain\n",
    "    task='text-generation',\n",
    "    device=device,\n",
    "    stopping_criteria = stopping_criteria,  \n",
    "    temperature = 0.1,\n",
    "    top_p = 0.15,  \n",
    "    top_k = 0,  \n",
    "    max_new_tokens = 200, # the number of tokens the model must generate\n",
    "    repetition_penalty = 1.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b665785-889f-40f7-9b45-ce42ac73178e",
   "metadata": {},
   "source": [
    "# Inferance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eff4e48f-be4a-4d68-b586-1049de280188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The machine shall automatically recognize the card regardless of its location.\n",
      "The machine button shall be clickable on the screen at least once if a valid key has been loaded. \n"
     ]
    }
   ],
   "source": [
    "prompt_1 = \"I want two usability requirements for bus ticket machines.\" \n",
    "\n",
    "result = pipe(f\"<s>[INST] {prompt_1} [/INST]\")\n",
    "print(result[0]['generated_text'].split(\"[/INST]\")[-1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0e36ecd-15b0-4a35-93c5-4f03d7e6caf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The machine shall automatically recognize the card regardless of its location.\n",
      "The machine button shall be clickable on the screen at least once if a valid key has been loaded. \n"
     ]
    }
   ],
   "source": [
    "prompt_2 = \"Write me three security requirements for an ATM software?\"\n",
    "\n",
    "result = pipe(f\"<s>[INST] {prompt_1} [/INST]\")\n",
    "print(result[0]['generated_text'].split(\"[/INST]\")[-1])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ddb86a-af46-4bbe-9f39-e9bc76e9f68b",
   "metadata": {},
   "source": [
    "## Try it for your self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4d62a21-1549-41fc-8b70-a09b2cf93f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below lines to use a custom query\n",
    "\n",
    "# your_prompt = 'your prompt goes here!'\n",
    "# result = pipe(f\"<s>[INST] {your_prompt} [/INST]\")\n",
    "# print(result[0]['generated_text'].split(\"[/INST]\")[-1])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd20e1db-7d0e-45a4-9e61-d84535870f18",
   "metadata": {},
   "source": [
    "# Model Setup with Langchain for Chat-based RE Elicitation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f8de78e-4ee7-4bfe-9326-86fd73b7001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing langchain required APIs\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf456ecc-f836-4b60-9fc7-2ed87e9b8253",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = HuggingFacePipeline(pipeline = pipe)\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key = \"history\",\n",
    "    k = 5,     # Chat history memory, increase/decrease it depending on how far the model should remember!\n",
    "    return_only_outputs = True)\n",
    "\n",
    "chat_history = ConversationChain(\n",
    "    llm = chat_model,\n",
    "    memory = memory,\n",
    "    verbose = True)\n",
    "\n",
    "chat_history.prompt.template = \"\"\"\n",
    "You are a professional requirements engineer who helps users brainstorm more software requirements.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "User: {input}\n",
    "Assistant:  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcb7b90f-a431-412f-a812-6ee2e5cebc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_re_elicitor(chat_chain, query):\n",
    "    chat_chain.predict(input = query)\n",
    "    last_message = chat_chain.memory.chat_memory.messages[-1]\n",
    "    last_message_content = last_message.content.split('\\n\\n')[0].strip()\n",
    "    \n",
    "    # cleaning prtompt text\n",
    "    prompt_id = last_message_content.find('\\n### Human:')\n",
    "    if prompt_id != -1:\n",
    "        last_message_content = last_message_content[:prompt_id]\n",
    "\n",
    "    # cleaning model generated text\n",
    "    for stop_token in ['***', '###', 'User:']:\n",
    "        last_message_content = last_message_content.removesuffix(stop_token)\n",
    "\n",
    "    return last_message_content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0cde6af-a0a1-492e-b5e0-dbad84524bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a professional requirements engineer who helps users brainstorm more software requirements.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "User: As a software engineer, I want to build an AI chatbot and integrate it with our school's adaptive learning platform. \n",
      "The chatbot should be capable of doing content search in the system, doing quizzes, motivating, real-time interaction with students, resolution of misunderstandings, and delivering curriculum content in various formats. \n",
      "It also needs the ability to provide topic definitions, connect with a recommendation system, and engage in small talk for a more interactive experience.\n",
      "I want you to brainstorm requirements for me.\n",
      "Assistant:  \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Sure! Based on your input, here is a list of five professionals teaching technologies (ET) courses at Carnegie Mellon University Pittsburgh Campus; has worked on projects involving natural language processing, sentiment analysis, spam detection, knowledge graph construction, question answering, text summarization, and machine translation. \n",
      "Another wants to use data from Carnegie Mellon University’s School of Computer Science’s Software Engineering Undergraduate Program surveys to predict student performance using Adaptive Learning Platform algorithms. \n",
      "Thirdly, he/she wants to construct an intelligent chatter bot that integrates with Carnegie Mellon University’s School of Computer Science’s Adaptive Learning Platform. The chatbot will perform the following functions: do content searches within the system, take quizzes, encourage students, resolve misunderstandings between students and teachery, deliver curriculum materials in diverse formatson end. Second\n"
     ]
    }
   ],
   "source": [
    "llm_re_elicitor(chat_history, '''As a software engineer, I want to build an AI chatbot and integrate it with our school's adaptive learning platform. \n",
    "The chatbot should be capable of doing content search in the system, doing quizzes, motivating, real-time interaction with students, resolution of misunderstandings, and delivering curriculum content in various formats. \n",
    "It also needs the ability to provide topic definitions, connect with a recommendation system, and engage in small talk for a more interactive experience.\n",
    "I want you to brainstorm requirements for me.''')\n",
    "print(chat_history.memory.chat_memory.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fe1c757-cf2a-4fcd-afe1-3d55b30c5f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a professional requirements engineer who helps users brainstorm more software requirements.\n",
      "\n",
      "Current conversation:\n",
      "Human: As a software engineer, I want to build an AI chatbot and integrate it with our school's adaptive learning platform. \n",
      "The chatbot should be capable of doing content search in the system, doing quizzes, motivating, real-time interaction with students, resolution of misunderstandings, and delivering curriculum content in various formats. \n",
      "It also needs the ability to provide topic definitions, connect with a recommendation system, and engage in small talk for a more interactive experience.\n",
      "I want you to brainstorm requirements for me.\n",
      "AI:  Sure! Based on your input, here is a list of five professionals teaching technologies (ET) courses at Carnegie Mellon University Pittsburgh Campus; has worked on projects involving natural language processing, sentiment analysis, spam detection, knowledge graph construction, question answering, text summarization, and machine translation. \n",
      "Another wants to use data from Carnegie Mellon University’s School of Computer Science’s Software Engineering Undergraduate Program surveys to predict student performance using Adaptive Learning Platform algorithms. \n",
      "Thirdly, he/she wants to construct an intelligent chatter bot that integrates with Carnegie Mellon University’s School of Computer Science’s Adaptive Learning Platform. The chatbot will perform the following functions: do content searches within the system, take quizzes, encourage students, resolve misunderstandings between students and teachery, deliver curriculum materials in diverse formatson end. Second\n",
      "User: 'Concerning requirement number 3, which you proposed in your previous message, tell me two methods I can employ to motivate students?\n",
      "Assistant:  \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Here are some examples of things you could say to stimulate discussion and generate additional ideas for software requirements. -- Requirements Engineer \n",
      "\n",
      "Recommendations... \n",
      "Gives 5 professionals teaching technology classes at Carnegie Mellon University Pittsburgh Campus have worked on natural language processing projects. \n",
      "1 wants to use undergraduate programming survey data to predict student performance using Adaptive Learning Platform algorithms. \n",
      "Wants to create an intelligent chatter botintegratedwith Carnegie Mellon University s SchoolofComputerscience SCS ALPS.  Chatbot shall perform thefollowingo operations :-- \n",
      "--- \n",
      "Use  of  CarnegieMellonUniversitySCSALPSSystemComponents. \n",
      "End_Of_Course_Survey = verify_data(undergradapplication. End_Of_Course_Surney \n",
      "--- \n",
      "2 Methodsof\n"
     ]
    }
   ],
   "source": [
    "llm_re_elicitor(chat_history, ''''Concerning requirement number 3, which you proposed in your previous message, tell me two methods I can employ to motivate students?''')\n",
    "print(chat_history.memory.chat_memory.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66d85cf-ec46-43c8-a470-435417a7299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your prtompt between ''' marks for continuation of the chat!\n",
    "\n",
    "# llm_re_elicitor(chat_history, '''' ''')\n",
    "# print(chat_history.memory.chat_memory.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a789e4-4f81-4574-9f6e-46ede29a1371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
