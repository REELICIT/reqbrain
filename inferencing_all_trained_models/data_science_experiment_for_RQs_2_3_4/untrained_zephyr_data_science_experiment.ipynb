{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b0fd7e8-79f6-4946-b2ef-d1802d7ef8af",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/REELICIT/reqbrain_rep_package/blob/4392bd86c062b49677fca2a59f5b9dd677eeb848/inferencing_all_trained_models/data_science_experiment_for_RQs_2_3_4/untrained_zephyr_data_science_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7d0f25-3887-4ca1-b6e5-a4bae8e02bd2",
   "metadata": {},
   "source": [
    "# Data Science Experiment\n",
    "\n",
    "- Results of this notebook is used for ***RQ2***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cc60d4-4635-49ec-9229-34bf9fff0436",
   "metadata": {},
   "source": [
    "# Zephyr 7b beta Download & Setup\n",
    "\n",
    "- this is the ***original Zephyr 7b beta, not trained on requirements elicitation task***\n",
    "- hugging face structure for each project is ```<user_name>/<model_name>```\n",
    "- unlike other notebooks we download the model for this notebook from its original owner.\n",
    "- therefore, variable ```model_name``` is starting wtih ***HuggingFaceH4***, see code line x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49be96bd-de36-48cd-8ecd-4f6048a8b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "\n",
    "# if not installed, use !pip install <name of the API>\n",
    "# in case of error due to lacking API:\n",
    "# Python writes the name and command to install the lacking API in the very last lines of the error message.\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch import cuda, bfloat16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f36930-b571-4723-862d-08a1fdbb2759",
   "metadata": {},
   "source": [
    "# Resources Checkup\n",
    "\n",
    "- **Important Note:** A GPU with minimum of 32GB GPU memory is required to load the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8affe20-b00f-424c-95b7-d6a30ecdc4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 1 Name: Tesla V100-SXM2-32GB\n",
      "GPU 1 Total Memory: 31.74 GB\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "\n",
    "    # Iterate over each GPU and print its name and memory information\n",
    "    for i in range(num_gpus):\n",
    "        gpu = torch.cuda.get_device_properties(i)\n",
    "        print(f\"GPU {i + 1} Name: {gpu.name}\")\n",
    "        print(f\"GPU {i + 1} Total Memory: {gpu.total_memory / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. A GPU with minimum of 32GB GPU memory is required to load the model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43908f3a-03c4-4bf3-91be-3332856e5247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detecting GPU device to load model on it\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c3b04e-13ce-4ed5-a8e7-3df5b34260e3",
   "metadata": {},
   "source": [
    "# Downloading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b304279-e44b-41e4-9e9b-15aafc1572d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e0cc9615534edabdda49cf4476e9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_name = 'HuggingFaceH4/zephyr-7b-beta'\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, use_fast = False)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c6da0d-0782-4a7d-8079-ce8dec5b4882",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abcdb1fb-447a-4033-81e1-2ff0bde8487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_token_ids = [\n",
    "    tokenizer.convert_tokens_to_ids(x) for x in [[tokenizer.eos_token], [\"<\", \"|\", \"user\"], [\"user\", \":\"], [\"|\", \"user\", \"|\", \">\"], [\"User\", \":\"]]\n",
    "]\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "106fa892-abd9-49fa-9dd3-4a27457a9281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c50a943-e82d-441b-a3ff-5aaa443b06e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 15:03:25.811634: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-22 15:03:25.846729: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-22 15:03:25.846761: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-22 15:03:25.846785: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-22 15:03:25.854731: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-22 15:03:26.970087: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "pipe = transformers.pipeline(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    return_full_text = True, # Set it to True when combining with LangChain\n",
    "    task = 'text-generation',\n",
    "    device = device,\n",
    "    stopping_criteria = stopping_criteria,  \n",
    "    temperature = 0.2,\n",
    "    top_p = 0.15,  \n",
    "    top_k = 0,  \n",
    "    max_new_tokens = 512,  \n",
    "    repetition_penalty = 1.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c75218-5945-44e7-8262-8b6f34a2f230",
   "metadata": {},
   "source": [
    "# Inferance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71b1612d-6123-41a7-8c29-a9730a4058c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"You are a professional requirements engineer who helps users brainstorm more software requirements.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d50c38-cb16-40f6-bbe4-35904de21308",
   "metadata": {},
   "source": [
    "## RQ2 (see table V: colum Untrained Model Requirements (Zephyr-7b-beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39d958c8-f2b0-4913-bb24-f17e4a42029a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.15` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Learning Goals: The chatbot should be able to identify the learning goals of individual students based on their academic background, previous performance in exams, and feedback from teachers. This will enable it to provide personalized recommendations for study materials, practice exercises, and quizzes that align with each student's specific needs (preferred goal). In the future, this feature could also allow the chatbot to adapt its teaching style and difficulty level according to how well the student is progressing towards these goals.\n",
      "2. Interactive Tutoring: The chatbot must have interactive tutoring capabilities that can simulate real classroom scenarios such as answering questions, explaining concepts, providing examples, and giving hints when needed. It should use natural language processing techniques to understand the context of the conversation and respond appropriately (non-mandatory requirement). Additionally, the chatbot may incorporate gamification elements like points, badges or leaderboards to make learning engaging and fun for students (future action).\n",
      "3. Feedback Mechanism: To enhance the effectiveness of the chatbot, there ought to be a mechanism through which students can give feedback about their experience using the tool. Based on this input, the system would analyze areas where improvements need to be made and adjust accordingly (required preference). Furthermore, the chatbot might offer suggestions for improvement to struggling students by identifying patterns in their mistakes and suggesting remedial activities (future action).\n",
      "4. Collaborative Learning: Aside from one-on-one interactions between students and the bot, the platform should support collaborative learning experiences via group discussions, peer review sessions, and online debates facilitated by the chatbot (optional preference). These features aim at fostering critical thinking skills among students while promoting teamwork and socialization.\n",
      "5. Data Privacy & Security: Lastly, since sensitive information regarding students’ educational history and preferences will be stored within the application, data privacy and security measures must be put in place to protect user data against unauthorized access, theft, misuse, disclosure, alteration, destruction, etc. (essential mandatory requirement).\n"
     ]
    }
   ],
   "source": [
    "rq2_chatbot_tutor = '''I want to build an AI Chatbot assistant to help students learn better.\n",
    "\n",
    "Write me 5 requirements for the chatbot.\n",
    "Make sure to include requirements indicating non-mandatory preferred goal and future actions too.'''\n",
    "\n",
    "result = result = pipe(f\"<|system|>\\n{instruction}\\n<|user|>\\n{rq2_chatbot_tutor}\\n<|assistant|>\\n\")\n",
    "print(result[0]['generated_text'].split(\"\\n<|assistant|>\\n\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9751b6-c61a-435f-a8cd-08f2b58c6656",
   "metadata": {},
   "source": [
    "### Seq. N, continue your own conversation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71151638-84e2-46d8-b7f4-35d2f5eaffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in between '''   ''' your text for continuation\n",
    "\n",
    "# llm_re_elicitor(chat_history, ''' ''')\n",
    "# print(chat_history.memory.chat_memory.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c439cc-c936-4cb7-a3e1-0e98712f3bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
