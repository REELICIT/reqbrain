{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a706923f-a517-489c-b197-17d296a77dd9",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/REELICIT/reqbrain_rep_package/blob/d6f1edc5fb5f41bdb4a6a9105b37c378ab4c96aa/inferencing_all_trained_models/all_models_used_with_langchain/falcon-instruct_inferance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a79f7d-c1c1-4d52-89b4-176dd9531f06",
   "metadata": {},
   "source": [
    "# ReqBrain (Falcon 7b instruct) Download & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49be96bd-de36-48cd-8ecd-4f6048a8b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "\n",
    "# if not installed, use !pip install <name of the API>\n",
    "# in case of error due to lacking API:\n",
    "# Python writes the name and command to install the lacking API in the very last lines of the error message.\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch import cuda, bfloat16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183054d5-154e-4feb-8029-d08973e15ec9",
   "metadata": {},
   "source": [
    "# Resources Checkup\n",
    "\n",
    "- **Important Note:** A GPU with minimum of 32GB GPU memory is required to load the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e32760d5-40b5-4797-8725-412ac1ab881f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 1 Name: Tesla V100-SXM2-32GB\n",
      "GPU 1 Total Memory: 31.74 GB\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "\n",
    "    # Iterate over each GPU and print its name and memory information\n",
    "    for i in range(num_gpus):\n",
    "        gpu = torch.cuda.get_device_properties(i)\n",
    "        print(f\"GPU {i + 1} Name: {gpu.name}\")\n",
    "        print(f\"GPU {i + 1} Total Memory: {gpu.total_memory / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. A GPU with minimum of 32GB GPU memory is required to load the model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "655a14c3-00fc-4329-be24-16477345dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detecting GPU device to load model on it\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a30c361-f080-4975-a944-45330d9de448",
   "metadata": {},
   "source": [
    "# Downloading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b304279-e44b-41e4-9e9b-15aafc1572d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915d056e490e4bcd80a251b4a8917721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_name = 'REELICIT/falcon-7b-instruct-ReqBrain'\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, use_fast = False)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b6f9c7-afdc-48b4-9e35-b5082158cf55",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abcdb1fb-447a-4033-81e1-2ff0bde8487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the list of stopping criteria for the model\n",
    "\n",
    "stop_token_ids = [\n",
    "    tokenizer.convert_tokens_to_ids(x) for x in [['Human', ':'], ['###', 'Human', ':'], [\"***\"], [\"###\"], [\"###\", \"Assistant\", \":\"], [tokenizer.eos_token]]\n",
    "]\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "106fa892-abd9-49fa-9dd3-4a27457a9281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c50a943-e82d-441b-a3ff-5aaa443b06e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 14:21:28.920135: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-19 14:21:28.969836: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-19 14:21:28.969871: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-19 14:21:28.969901: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-19 14:21:28.983538: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-19 14:21:30.562182: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "pipe = transformers.pipeline(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    return_full_text = True, # Set it to True when combining with LangChain\n",
    "    task = 'text-generation',\n",
    "    device = device,\n",
    "    stopping_criteria = stopping_criteria,  \n",
    "    temperature = 0.1,\n",
    "    top_p = 0.15,  \n",
    "    top_k = 0,  \n",
    "    max_new_tokens = 200,  # the number of tokens the model must generate\n",
    "    repetition_penalty = 1.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402ba9c0-b5eb-436a-884a-7054247ad5c9",
   "metadata": {},
   "source": [
    "# Inferance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eff4e48f-be4a-4d68-b586-1049de280188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.15` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The first requirement, NP3-005-1, states that the system must provide a mechanism to store personal information such as name and address. This functionality should allow the individual to anonymously use the machine without revealing their identity if desired. The second requirement, NP3-005-2, outlines a provision allowing the insertion of credit or debit card numbers by numeric keystrokes rather than using symbols for each transaction. It must be possible to make changes or transactions quickly while still seated at the computer. Together, these requirements outline four distinct aspects pertaining to user interface and usage guidelines. These are just some examples of usability requirements; additional specifics can be found in ISO 29148 recommendations. Examples include setting up default preferences, utilizing existing data and providing options for future uses based on past interactions.\n",
      "The given requirement is not specific enough to be considered as a concrete requirement regarding usability. A requirement like this would require consideration of factors related to ease of use, specifically relating to navigation and interaction with the\n"
     ]
    }
   ],
   "source": [
    "prompt_1 = \"I want two usability requirements for bus ticket machines.\" \n",
    "\n",
    "result = pipe(f\"### Human: {prompt_1}.### Assistant: \")\n",
    "print(result[0]['generated_text'].split('### Assistant: ')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0e36ecd-15b0-4a35-93c5-4f03d7e6caf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Security Regulation Requirement RS-1 specifies that the system must implement secure passwords or biometric authentication methods to authorize access to account data. Other Requries include Secure Connection Protocol and SSL Certificate Validation, both of which aim to ensure secure communication between the client and the server. Additionally, the Use Access Control subclass Restriction RUSC2 restricts access to transaction details to authorized users only.###\n"
     ]
    }
   ],
   "source": [
    "prompt_2 = \"Write me three security requirements for an ATM software?\"\n",
    "\n",
    "result = pipe(f\"### Human: {prompt_2}.### Assistant: \")\n",
    "print(result[0]['generated_text'].split('### Assistant: ')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8a3e5c-0b73-4e5b-ad9d-f55111b2115e",
   "metadata": {},
   "source": [
    "## Try it for your self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70ba22f2-ca87-4692-9d0e-a1195d8a6da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below lines to use a custom query\n",
    "\n",
    "# your_prompt = 'your prompt goes here!'\n",
    "# result = pipe(f\"### Human: {your_prompt}.### Assistant: \")\n",
    "# print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a9210-15f4-406e-b4bd-e35959201ff1",
   "metadata": {},
   "source": [
    "# Model Setup with Langchain for Chat-based RE Elicitation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f8de78e-4ee7-4bfe-9326-86fd73b7001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing langchain required APIs\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf456ecc-f836-4b60-9fc7-2ed87e9b8253",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = HuggingFacePipeline(pipeline = pipe)\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key = \"history\",\n",
    "    k = 5,     # Chat history memory, increase/decrease it depending on how far the model should remember!\n",
    "    return_only_outputs = True)\n",
    "\n",
    "chat_history = ConversationChain(\n",
    "    llm = chat_model,\n",
    "    memory = memory,\n",
    "    verbose = True)\n",
    "\n",
    "chat_history.prompt.template = \"\"\"\n",
    "You are a professional requirements engineer who helps users brainstorm more software requirements.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "### Human: {input}\n",
    "### Assistant: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcb7b90f-a431-412f-a812-6ee2e5cebc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_re_elicitor(chat_chain, query):\n",
    "    chat_chain.predict(input = query)\n",
    "    last_message = chat_chain.memory.chat_memory.messages[-1]\n",
    "    last_message_content = last_message.content.split('\\n\\n')[0].strip()\n",
    "    \n",
    "    # cleaning prtompt text\n",
    "    prompt_id = last_message_content.find('\\n### Human:')\n",
    "    if prompt_id != -1:\n",
    "        last_message_content = last_message_content[:prompt_id]\n",
    "\n",
    "    # cleaning model generated text\n",
    "    for stop_token in ['***', '###', '### Assistant:', '### Human:', 'Human:']:\n",
    "        last_message_content = last_message_content.removesuffix(stop_token)\n",
    "\n",
    "    return last_message_content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0cde6af-a0a1-492e-b5e0-dbad84524bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.15` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a professional requirements engineer who helps users brainstorm more software requirements.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "### Human: As a software engineer, I want to build an AI chatbot and integrate it with our school's adaptive learning platform. \n",
      "The chatbot should be capable of doing content search in the system, doing quizzes, motivating, real-time interaction with students, resolution of misunderstandings, and delivering curriculum content in various formats. \n",
      "It also needs the ability to provide topic definitions, connect with a recommendation system, and engage in small talk for a more interactive experience.\n",
      "I want you to brainstorm requirements for me.\n",
      "### Assistant: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "As a user, I would like to have a feature where the AI chatbot can generate personalized lesson plans based on each student's individual learning needs.\n",
      "\n",
      "Requirements:\n",
      "\n",
      "- The AI chatbot shall be able to create individualized lesson plans based on each student's learning needs.\n",
      "- The chatbot should be integrated with the school's adaptive learning platform, allowing creation of customized lesson plans tailored to each student's learning needs.\n",
      "- It should interface with the recommendation engine to allow generation of personalized lesson plans based on each student's individual learning needs.\n",
      "- The chatbot shall interact with each student individually, providing them with personalized learning plans that are tailored to their individual learning needs.\n",
      "- The system should be able to track each student's progress and record their interactions with the chatbot.\n",
      "- The chatbot should be able to create individualized learning plans based on each student's learning needs.\n",
      "- The system should be integrated with the school'\n"
     ]
    }
   ],
   "source": [
    "llm_re_elicitor(chat_history, '''As a software engineer, I want to build an AI chatbot and integrate it with our school's adaptive learning platform. \n",
    "The chatbot should be capable of doing content search in the system, doing quizzes, motivating, real-time interaction with students, resolution of misunderstandings, and delivering curriculum content in various formats. \n",
    "It also needs the ability to provide topic definitions, connect with a recommendation system, and engage in small talk for a more interactive experience.\n",
    "I want you to brainstorm requirements for me.''')\n",
    "print(chat_history.memory.chat_memory.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13463585-ed8c-40fb-9568-624aa98c5024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.15` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a professional requirements engineer who helps users brainstorm more software requirements.\n",
      "\n",
      "Current conversation:\n",
      "Human: As a software engineer, I want to build an AI chatbot and integrate it with our school's adaptive learning platform. \n",
      "The chatbot should be capable of doing content search in the system, doing quizzes, motivating, real-time interaction with students, resolution of misunderstandings, and delivering curriculum content in various formats. \n",
      "It also needs the ability to provide topic definitions, connect with a recommendation system, and engage in small talk for a more interactive experience.\n",
      "I want you to brainstorm requirements for me.\n",
      "AI: \n",
      "As a user, I would like to have a feature where the AI chatbot can generate personalized lesson plans based on each student's individual learning needs.\n",
      "\n",
      "Requirements:\n",
      "\n",
      "- The AI chatbot shall be able to create individualized lesson plans based on each student's learning needs.\n",
      "- The chatbot should be integrated with the school's adaptive learning platform, allowing creation of customized lesson plans tailored to each student's learning needs.\n",
      "- It should interface with the recommendation engine to allow generation of personalized lesson plans based on each student's individual learning needs.\n",
      "- The chatbot shall interact with each student individually, providing them with personalized learning plans that are tailored to their individual learning needs.\n",
      "- The system should be able to track each student's progress and record their interactions with the chatbot.\n",
      "- The chatbot should be able to create individualized learning plans based on each student's learning needs.\n",
      "- The system should be integrated with the school'\n",
      "### Human: 'Concerning requirement number 3, which you proposed in your previous message, tell me two methods I can employ to motivate students?\n",
      "### Assistant: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "'The AI chatbot will generate personalized learning plans based on each student's individual learning needs. This feature will enable the system to track each student's progress and record their interactions with the chatbot. \n",
      "1. Offering rewards or incentives for completing tasks.\n",
      "2. Providing rewards or incentives for completing tasks.\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "llm_re_elicitor(chat_history, ''''Concerning requirement number 3, which you proposed in your previous message, tell me two methods I can employ to motivate students?''')\n",
    "print(chat_history.memory.chat_memory.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "415839f6-d50a-4b55-a08d-e5686baf37f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your prtompt between ''' marks for continuation of the chat!\n",
    "\n",
    "# llm_re_elicitor(chat_history, '''' ''')\n",
    "# print(chat_history.memory.chat_memory.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eba60a-b18a-4cad-bb11-b4cdbda849a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
