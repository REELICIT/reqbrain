{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62f295e5-f16e-4343-a733-ef61a294446d",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/[USER]/[REPO]/blob/[BRANCH]/[NOTEBOOK_FILE].ipynb" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0927387e-4ceb-494c-88be-28c1a875be92",
   "metadata": {},
   "source": [
    "# Common imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de20d6fa-b40c-4f2d-999f-a99404e841e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-01 11:08:40.338414: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-01 11:08:40.376787: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-01 11:08:40.376816: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-01 11:08:40.376839: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-01 11:08:40.384962: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-01 11:08:41.592691: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a01c504-1973-4d82-be87-cf47f78dc2bc",
   "metadata": {},
   "source": [
    "# Loading the Instruct Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b2109b1-99d4-4d7e-9c46-9a242a99bb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['REQID_ex', 'completion', 'query', 'class', 'task', 'text', 'label', 'mistral_ai_instruct_7b_chat_hf_preds', 'falcon_7b_base_preds', 'falcon_7b_instruct_preds', 'llama2_7b_chat_hf_preds', 'zephyr_7b_beta_preds'],\n",
       "    num_rows: 34\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_from_disk('./predicting_with_models/models_prediction_dataset')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad43501e-51bd-475a-8533-d0c917e37eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the human written requirements\n",
    "\n",
    "references = dataset['completion']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39293bd9-1a97-4c0d-ac5f-18f7d3b81db0",
   "metadata": {},
   "source": [
    "# Putting all Metrics Togather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f8755cc-683f-4595-897d-31ab7174e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(references, predictions):\n",
    "    bertscore = evaluate.load('bertscore')\n",
    "    frugalscore = evaluate.load(\"frugalscore\", \"moussaKam/frugalscore_medium_roberta_bert-score\")\n",
    "\n",
    "    bertscore_results = bertscore.compute(predictions = predictions, references = references, model_type = \"xlm-mlm-en-2048\", lang = 'en')\n",
    "    frugalscore_results = frugalscore.compute(predictions=predictions, references=references, batch_size = 16, max_length = 512, device = \"gpu\")\n",
    "    \n",
    "    return {'bert_score': bertscore_results, 'frugal_score': frugalscore_results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2646fcd5-c52a-40d7-b2e2-161482e5fa1c",
   "metadata": {},
   "source": [
    "# Evaluating Trained Models using Five NLP Human Correlation Metrics\n",
    "\n",
    "- results of below cells are compiled on table II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec273d9-6f85-42b2-b871-d52525b70d87",
   "metadata": {},
   "source": [
    "## Evaluating Zephyr 7b beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91642738-b8b7-47ee-a1ed-3c7730a289d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4dfd891aa14c87a24b4a4dd2f0a381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = evaluate_model(references, dataset['zephyr_7b_beta_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04f9b4f9-9921-466b-8195-5b7da0c439db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to generate a two formated columns for BERT and FRUGAL to be used for SPIDER chart on paper\n",
    "\n",
    "# zephyr_frugal_score = results['frugal_score']['scores']\n",
    "# zephyr_bert_score = results['bert_score']['recall']\n",
    "# dataset_for_spider_chart = dataset.add_column('zephyr_frugal_score', zephyr_frugal_score)\n",
    "# dataset_for_spider_chart = dataset_for_spider_chart.add_column('zephyr_bert_score', zephyr_bert_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d1c5ec2-71eb-4828-8925-c3d9885f3b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m precision:\u001b[0m \t 0.8904935437090257\n",
      "\u001b[1m recall:\u001b[0m \t 0.8960549077566933\n",
      "\u001b[1m f1:\u001b[0m \t 0.8930980913779315\n",
      "......................................................................................................................................................\n",
      "\u001b[1m FRUGAL Score:\u001b[0m \t 0.9120519301470589\n",
      "......................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "for metric in list(results['bert_score'].keys())[:-1]:\n",
    "    pairwise_metric = results['bert_score'][metric]\n",
    "    averaged_metric = np.sum(pairwise_metric)/len(pairwise_metric)\n",
    "    print(f'\\033[1m {metric}:\\033[0m \\t', averaged_metric)\n",
    "print('.' * 150)\n",
    "\n",
    "pairwise_frugal_score = results['frugal_score']['scores']\n",
    "averaged_frugal_score = np.sum(results['frugal_score']['scores'])/len(results['frugal_score']['scores'])\n",
    "print('\\033[1m FRUGAL Score:\\033[0m \\t', averaged_frugal_score)\n",
    "print('.' * 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e318b-e017-424b-92ad-6d7b0165625c",
   "metadata": {},
   "source": [
    "## Evaluating Mistralai 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f39213f-e644-4593-b33f-9822e7270354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf96d24577d4c2aae507b4c9b45740b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = evaluate_model(references, dataset['mistral_ai_instruct_7b_chat_hf_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "008a81c4-985f-4160-9570-ab4c617bda88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to generate a two formated columns for BERT and FRUGAL to be used for SPIDER chart on paper\n",
    "\n",
    "# mistralai_frugal_score = results['frugal_score']['scores']\n",
    "# mistralai_bert_score = results['bert_score']['recall']\n",
    "# dataset_for_spider_chart = dataset_for_spider_chart.add_column('mistralai_frugal_score', mistralai_frugal_score)\n",
    "# dataset_for_spider_chart = dataset_for_spider_chart.add_column('mistralai_bert_score', mistralai_bert_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c94bd65b-427f-4c06-9e61-33bc60f04005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m precision:\u001b[0m \t 0.8448562324047089\n",
      "\u001b[1m recall:\u001b[0m \t 0.8912202558096718\n",
      "\u001b[1m f1:\u001b[0m \t 0.8671604254666496\n",
      "......................................................................................................................................................\n",
      "\u001b[1m FRUGAL Score:\u001b[0m \t 0.8881405101102942\n",
      "......................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "for metric in list(results['bert_score'].keys())[:-1]:\n",
    "    pairwise_metric = results['bert_score'][metric]\n",
    "    averaged_metric = np.sum(pairwise_metric)/len(pairwise_metric)\n",
    "    print(f'\\033[1m {metric}:\\033[0m \\t', averaged_metric)\n",
    "print('.' * 150)\n",
    "\n",
    "pairwise_frugal_score = results['frugal_score']['scores']\n",
    "averaged_frugal_score = np.sum(results['frugal_score']['scores'])/len(results['frugal_score']['scores'])\n",
    "print('\\033[1m FRUGAL Score:\\033[0m \\t', averaged_frugal_score)\n",
    "print('.' * 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8ce1e5-dced-4005-bb9c-845499cca589",
   "metadata": {},
   "source": [
    "## Evaluating Falcon Base 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e18ade02-d081-48ae-a928-7637ab4ad8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2208de675b1d4f9496a8a29b08edfcff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = evaluate_model(references, dataset['falcon_7b_base_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c08ecc3e-9fed-4386-84f7-ddece6a32f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to generate a two formated columns for BERT and FRUGAL to be used for SPIDER chart on paper\n",
    "\n",
    "# falcon_base_frugal_score = results['frugal_score']['scores']\n",
    "# falcon_base_bert_score = results['bert_score']['recall']\n",
    "# dataset_for_spider_chart = dataset_for_spider_chart.add_column('falcon_base_frugal_score', falcon_base_frugal_score)\n",
    "# dataset_for_spider_chart = dataset_for_spider_chart.add_column('falcon_base_bert_score', falcon_base_bert_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae462d84-dac2-4ea4-907a-088875d56ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m precision:\u001b[0m \t 0.803386002779007\n",
      "\u001b[1m recall:\u001b[0m \t 0.8234570254297817\n",
      "\u001b[1m f1:\u001b[0m \t 0.8587363730458653\n",
      "......................................................................................................................................................\n",
      "\u001b[1m FRUGAL Score:\u001b[0m \t 0.8855554917279411\n",
      "......................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "for metric in list(results['bert_score'].keys())[:-1]:\n",
    "    pairwise_metric = results['bert_score'][metric]\n",
    "    averaged_metric = np.sum(pairwise_metric)/len(pairwise_metric)\n",
    "    print(f'\\033[1m {metric}:\\033[0m \\t', averaged_metric)\n",
    "print('.' * 150)\n",
    "\n",
    "pairwise_frugal_score = results['frugal_score']['scores']\n",
    "averaged_frugal_score = np.sum(results['frugal_score']['scores'])/len(results['frugal_score']['scores'])\n",
    "print('\\033[1m FRUGAL Score:\\033[0m \\t', averaged_frugal_score)\n",
    "print('.' * 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a7c0b6-cd64-4ddc-88f1-534c7c9bb035",
   "metadata": {},
   "source": [
    "## Evaluating Falcon Instruct 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d95274be-6df8-40bb-8615-cecfb05d8290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bec759a06d440e6aa20ad692098a4ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = evaluate_model(references, dataset['falcon_7b_instruct_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66b6087d-d9f6-48ab-ad83-9eb3d496894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to generate a two formated columns for BERT and FRUGAL to be used for SPIDER chart on paper\n",
    "\n",
    "# falcon_frugal_score = results['frugal_score']['scores']\n",
    "# falcon_bert_score = results['bert_score']['recall']\n",
    "# dataset_for_spider_chart = dataset_for_spider_chart.add_column('falcon_frugal_score', falcon_frugal_score)\n",
    "# dataset_for_spider_chart = dataset_for_spider_chart.add_column('falcon_bert_score', falcon_bert_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f2634d4-65b4-4728-be36-ec09bf6a73b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m precision:\u001b[0m \t 0.8550121679025537\n",
      "\u001b[1m recall:\u001b[0m \t 0.8839603311875287\n",
      "\u001b[1m f1:\u001b[0m \t 0.8689493729787714\n",
      "......................................................................................................................................................\n",
      "\u001b[1m FRUGAL Score:\u001b[0m \t 0.8858570772058824\n",
      "......................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "for metric in list(results['bert_score'].keys())[:-1]:\n",
    "    pairwise_metric = results['bert_score'][metric]\n",
    "    averaged_metric = np.sum(pairwise_metric)/len(pairwise_metric)\n",
    "    print(f'\\033[1m {metric}:\\033[0m \\t', averaged_metric)\n",
    "print('.' * 150)\n",
    "\n",
    "pairwise_frugal_score = results['frugal_score']['scores']\n",
    "averaged_frugal_score = np.sum(results['frugal_score']['scores'])/len(results['frugal_score']['scores'])\n",
    "print('\\033[1m FRUGAL Score:\\033[0m \\t', averaged_frugal_score)\n",
    "print('.' * 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22056cf-7876-48f0-b9e8-2ae650dc0534",
   "metadata": {},
   "source": [
    "## Evaluating Llama2 7b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34fbe6ab-8314-4fba-877d-8d82b3fa31de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947988c9d5b64dfb80ee49442c988b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = evaluate_model(references, dataset['llama2_7b_chat_hf_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aeeae21e-4c63-4a93-91e7-fb82fc305d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to generate a two formated columns for BERT and FRUGAL to be used for SPIDER chart on paper\n",
    "\n",
    "# llama_frugal_score = results['frugal_score']['scores']\n",
    "# llama_bert_score = results['bert_score']['recall']\n",
    "# dataset_for_spider_chart = dataset_for_spider_chart.add_column('llama_frugal_score', llama_frugal_score)\n",
    "# dataset_for_spider_chart = dataset_for_spider_chart.add_column('llama_bert_score', llama_bert_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d811be67-2c3e-4277-a506-8b20b1d60c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m precision:\u001b[0m \t 0.8162222487085006\n",
      "\u001b[1m recall:\u001b[0m \t 0.8587226394344779\n",
      "\u001b[1m f1:\u001b[0m \t 0.85971639436834\n",
      "......................................................................................................................................................\n",
      "\u001b[1m FRUGAL Score:\u001b[0m \t 0.8812471277573529\n",
      "......................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "for metric in list(results['bert_score'].keys())[:-1]:\n",
    "    pairwise_metric = results['bert_score'][metric]\n",
    "    averaged_metric = np.sum(pairwise_metric)/len(pairwise_metric)\n",
    "    print(f'\\033[1m {metric}:\\033[0m \\t', averaged_metric)\n",
    "print('.' * 150)\n",
    "\n",
    "pairwise_frugal_score = results['frugal_score']['scores']\n",
    "averaged_frugal_score = np.sum(results['frugal_score']['scores'])/len(results['frugal_score']['scores'])\n",
    "print('\\033[1m FRUGAL Score:\\033[0m \\t', averaged_frugal_score)\n",
    "print('.' * 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78aa7705-fbb1-4aa1-8bb7-ccecc8a9b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving dataset with special columns for spider chart\n",
    "# dataset_for_spider_chart.save_to_disk('./dataset_for_spider_chart')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59968f5-e6e8-40eb-bb61-0e3336e3ebf1",
   "metadata": {},
   "source": [
    "# Evaluating Chatbot Requirements with Zephyr Generated ones\n",
    "\n",
    "- Results are used for BERT and FRUGAL scores are used for on the paper ***RQ-4***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "131e261d-f7e0-4d2c-827f-df6dd8eb7056",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_chatbot_reqs = [\"The chatbot shall be able to interface with the content search mechanism/index to provide students with pointers to materials.\",\n",
    "                      \"The chatbot shall be capable of acting as a quiz master.\",\n",
    "                      \"The chatbot shall motivate students.\",\n",
    "                      \"The chatbot should resolve misunderstandings for user intents.\",\n",
    "                      \"The chatbot shall provide its services in real time.\",\n",
    "                      \"The chatbot shall be able to deliver certain answers/content from the curriculum in three formats, i.e., text, video and pictures.\",\n",
    "                      \"The chatbot shall provide definitions for a topic if asked for in FAQ-style.\",\n",
    "                      \"The chatbot should interface with recommender.\",\n",
    "                      \"The chatbot should be capable of small talk.\"]\n",
    "                      \n",
    "zephyr_generated_reqs = [\"The chatbot shall perform content search in the Learning Management System (LMS).\",\n",
    "                     \"The chatbot shall carry out Quizzes.\",\n",
    "                     \"The Chatbot shall Motivate Students.\",\n",
    "                     \"The Chatbot shall resolve Misunderstandings.\",\n",
    "                     \"The Chatbot shall have Real-Time Interaction With Students.\",\n",
    "                     \"The Chatbot shall deliver Curriculum Content in Various Formats.\",\n",
    "                     \"The Chatbot shall Provide Topic Definitions.\",\n",
    "                     \"The Chatbot shall Connect To A Recommendation System.\",\n",
    "                     \"The Chatbot shall Engage In Small Talk.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "303ae220-e120-42bb-961e-99813490a07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be9de4c240942e09358b3ae7bf08c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = evaluate_model(original_chatbot_reqs, zephyr_generated_reqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a072c9e0-74de-46f3-9ebd-f835427ae042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m precision:\u001b[0m \t 0.9397746788130866\n",
      "\u001b[1m recall:\u001b[0m \t 0.9130040208498637\n",
      "\u001b[1m f1:\u001b[0m \t 0.9259782565964593\n",
      "......................................................................................................................................................\n",
      "\u001b[1m FRUGAL Score:\u001b[0m \t 0.9441189236111112\n",
      "......................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "for metric in list(results['bert_score'].keys())[:-1]:\n",
    "    pairwise_metric = results['bert_score'][metric]\n",
    "    averaged_metric = np.sum(pairwise_metric)/len(pairwise_metric)\n",
    "    print(f'\\033[1m {metric}:\\033[0m \\t', averaged_metric)\n",
    "print('.' * 150)\n",
    "\n",
    "pairwise_frugal_score = results['frugal_score']['scores']\n",
    "averaged_frugal_score = np.sum(results['frugal_score']['scores'])/len(results['frugal_score']['scores'])\n",
    "print('\\033[1m FRUGAL Score:\\033[0m \\t', averaged_frugal_score)\n",
    "print('.' * 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f356f3d3-08cc-4dc1-a2af-ddfe8a1e5919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal-engine",
   "language": "python",
   "name": "metal-engine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
