{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b0fd7e8-79f6-4946-b2ef-d1802d7ef8af",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/REELICIT/reqbrain_rep_package/blob/db6c7a1f2a35b5a50a6ad3912b69b5b57ed08e43/inferencing_all_trained_models/data_science_experiment_for_RQs_2_3_4/ReqBrain_zephyr_data_science_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  }, 
  {
   "cell_type": "markdown",
   "id": "c142dca7-a025-4ca7-9bb9-bc517d882308",
   "metadata": {},
   "source": [
    "# Data Science Experiment\n",
    "\n",
    "- Results of this notebook is used for ***RQs*** ***2***, ***3***, and ***4***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47c60e8-3c19-498b-be1d-0956c9991500",
   "metadata": {},
   "source": [
    "# ReqBrain (Zephyr 7b beta) Download & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49be96bd-de36-48cd-8ecd-4f6048a8b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "\n",
    "# if not installed, use !pip install <name of the API>\n",
    "# in case of error due to lacking API:\n",
    "# Python writes the name and command to install the lacking API in the very last lines of the error message.\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch import cuda, bfloat16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593eb4d5-fcc8-4a33-b38e-f141c8617dae",
   "metadata": {},
   "source": [
    "# Resources Checkup\n",
    "\n",
    "- **Important Note:** A GPU with minimum of 32GB GPU memory is required to load the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37d11614-bb31-46a5-909b-32cb2d7df975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 1 Name: Tesla V100-SXM2-32GB\n",
      "GPU 1 Total Memory: 31.74 GB\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "\n",
    "    # Iterate over each GPU and print its name and memory information\n",
    "    for i in range(num_gpus):\n",
    "        gpu = torch.cuda.get_device_properties(i)\n",
    "        print(f\"GPU {i + 1} Name: {gpu.name}\")\n",
    "        print(f\"GPU {i + 1} Total Memory: {gpu.total_memory / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. A GPU with minimum of 32GB GPU memory is required to load the model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d823024-0f3d-470b-a432-da8f010664d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detecting GPU device to load model on it\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c3b04e-13ce-4ed5-a8e7-3df5b34260e3",
   "metadata": {},
   "source": [
    "# Downloading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b304279-e44b-41e4-9e9b-15aafc1572d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4cea3fb84dd4bc68c4762ad4f37f6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_name = 'REELICIT/zephyr-7b-beta-ReqBrain'\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, use_fast = False)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef41d381-ece0-4cce-8eb2-1e01f0031852",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abcdb1fb-447a-4033-81e1-2ff0bde8487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_token_ids = [\n",
    "    tokenizer.convert_tokens_to_ids(x) for x in [[tokenizer.eos_token], [\"<\", \"|\", \"user\"], [\"user\", \":\"], [\"|\", \"user\", \"|\", \">\"], [\"User\", \":\"]]\n",
    "]\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "106fa892-abd9-49fa-9dd3-4a27457a9281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c50a943-e82d-441b-a3ff-5aaa443b06e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 14:37:38.008948: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-22 14:37:38.054807: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-22 14:37:38.054841: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-22 14:37:38.054865: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-22 14:37:38.073104: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-22 14:37:39.666287: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "pipe = transformers.pipeline(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    return_full_text = True, # Set it to True when combining with LangChain\n",
    "    task='text-generation',\n",
    "    device = device,\n",
    "    stopping_criteria = stopping_criteria,  \n",
    "    temperature = 0.2,\n",
    "    top_p = 0.15,  \n",
    "    top_k = 0,  \n",
    "    max_new_tokens = 512,  \n",
    "    repetition_penalty = 1.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb90163-e8aa-4eee-9f4b-9e781871d8d9",
   "metadata": {},
   "source": [
    "# Inferance for RQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71b1612d-6123-41a7-8c29-a9730a4058c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"You are a professional requirements engineer who helps users brainstorm more software requirements.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d50c38-cb16-40f6-bbe4-35904de21308",
   "metadata": {},
   "source": [
    "## RQ2 (table V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39d958c8-f2b0-4913-bb24-f17e4a42029a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.15` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RQ1: The chatbot shall provide personalized study suggestions based on the studentâ€™s course history and learning style.\n",
      "RQ2: The chatbot shall respond to queries related to academic courses and campus life within 3 seconds of response time.\n",
      "RQ3: The chatbot's responses to student inquiries will be accurate 95% of the time.\n",
      "RQ4: The chatbot will be integrated with the university system to access up-to-date information about academics, campuses, accommodation, and other relevant topics.\n",
      "RQ5: In case the chatbot is unable to understand or resolve a student query, it should handover the conversation to a human support agent.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rq2_chatbot_tutor = '''I want to build an AI Chatbot assistant to help students learn better.\n",
    "\n",
    "Write me 5 requirements for the chatbot.\n",
    "Make sure to include requirements indicating non-mandatory preferred goal and future actions too.'''\n",
    "\n",
    "rq2_result = pipe(f\"<|system|>\\n{instruction}\\n<|user|>\\n{rq2_chatbot_tutor}\\n<|assistant|>\\n\")\n",
    "print(rq2_result[0]['generated_text'].split(\"\\n<|assistant|>\\n\")[-1].strip('<|user'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62fef09-5f45-4c6d-99ca-a9ff0b4598f7",
   "metadata": {},
   "source": [
    "## RQ3 (table VI: column ReqBrain Generated Requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b082678-d9e4-4bfb-8c29-ad83c261c9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are some more requirements:\n",
      "\n",
      "1 - The chatbot shall identify whether a user question falls into a known query category or not.\n",
      "2 - In case the user question is not familiar, the chatbot shall direct the user to the support staff.\n",
      "3 - The chatbot shall ensure the privacy of student data.\n",
      "4 - The chatbot shall be able to suggest remedial exercises based on past performances.\n",
      "5 - The chatbot shall provide feedback to teachers.\n",
      "6 - The chatbot shall improve over time through learning from conversations.\n",
      "7 - The chatbot shall work 24 hours a day.\n",
      "8 - Some messages delivered by the chatbot shall be saved in a log for analysis purposes.\n",
      "9 - The chatbot shall communicate via instant messaging.\n"
     ]
    }
   ],
   "source": [
    "rq3_chatbot_tutor = '''Below is the list of requirements we collected for an AI Chatbot to help school students.\n",
    "1 - The chatbot shall be able to interface with the content search mechanism/index to provide students with pointers to materials.\n",
    "2 - The chatbot shall be capable of acting as a quiz master.\n",
    "3 - The chatbot shall motivate students.\n",
    "4 - The chatbot should resolve misunderstandings for user intents.\n",
    "5 - The chatbot shall provide its services in real time.\n",
    "6 - The chatbot shall be able to deliver certain answers/content from the curriculum in three formats, i.e., text, video and pictures.\n",
    "7 - The chatbot shall provide definitions for a topic if asked for in FAQ-style.\n",
    "8 - The chatbot should interface with recommender.\n",
    "9 - The chatbot should be capable of small talk.\n",
    "\n",
    "Can you give a few requirements, that are missing, for the chatbot?'''\n",
    "\n",
    "rq3_result = result = pipe(f\"<|system|>\\n{instruction}\\n<|user|>\\n{rq3_chatbot_tutor}\\n<|assistant|>\\n\")\n",
    "print(rq3_result[0]['generated_text'].split(\"\\n<|assistant|>\\n\")[-1].strip('<|user'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb11a15-96ba-407d-8e30-5e472135824d",
   "metadata": {},
   "source": [
    "## RQ4 (table VII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f8de78e-4ee7-4bfe-9326-86fd73b7001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing langchain required APIs\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf456ecc-f836-4b60-9fc7-2ed87e9b8253",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = HuggingFacePipeline(pipeline = pipe)\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key = \"history\",\n",
    "    k = 5,     # Chat history memory, increase/decrease it depending on how far the model should remember!\n",
    "    return_only_outputs = True)\n",
    "\n",
    "chat_history = ConversationChain(\n",
    "    llm = chat_model,\n",
    "    memory = memory,\n",
    "    verbose = True)\n",
    "\n",
    "chat_history.prompt.template = f\"{instruction}\\n\" + \"\"\"\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "user: {input}\n",
    "assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcb7b90f-a431-412f-a812-6ee2e5cebc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_re_elicitor(chat_chain, query):\n",
    "    chat_chain.predict(input = query)\n",
    "    last_message = chat_chain.memory.chat_memory.messages[-1]\n",
    "    last_message_content = last_message.content.split('\\n\\n')[0].strip()\n",
    "    \n",
    "    # cleaning prtompt text\n",
    "    prompt_id = last_message_content.find('\\nuser:')\n",
    "    if prompt_id != -1:\n",
    "        last_message_content = last_message_content[:prompt_id]\n",
    "\n",
    "    # cleaning model generated text\n",
    "    for stop_token in ['user:', 'assistant:']:\n",
    "        last_message_content = last_message_content.removesuffix(stop_token)\n",
    "\n",
    "    return last_message_content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdf7c67-bd93-4c66-9083-732679decf29",
   "metadata": {},
   "source": [
    "### Seq. #1 (shown on table VII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0cde6af-a0a1-492e-b5e0-dbad84524bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a professional requirements engineer who helps users brainstorm more software requirements.\n",
      "\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "user: As a software engineer, I want to build an AI chatbot and integrate it with our school's adaptive learning platform. \n",
      "The chatbot should be capable of doing content search in the system, doing quizzes, motivating, real-time interaction with students, resolution of misunderstandings, and delivering curriculum content in various formats. \n",
      "It also needs the ability to provide topic definitions, connect with a recommendation system, and engage in small talk for a more interactive experience.\n",
      "I want you to brainstorm requirements for me.\n",
      "assistant:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.15` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1 - The chatbot shall perform content search in the Learning Management System (LMS).\n",
      "2 - The chatbot shall carry out Quizzes.\n",
      "3 - The Chatbot shall Motivate Students.\n",
      "4 - The Chatbot shall have Real-Time Interaction With Students.\n",
      "5 - The Chatbot shall resolve Misunderstandings.\n",
      "6 - The Chatbot shall deliver Curriculum Content in Various Formats.\n",
      "7 - The Chatbot shall Provide Topic Definitions.\n",
      "8 - The Chatbot shall Connect To A Recommendation System.\n",
      "9 - The Chatbot shall Engage In Small Talk.\n",
      "user:\n"
     ]
    }
   ],
   "source": [
    "llm_re_elicitor(chat_history, '''As a software engineer, I want to build an AI chatbot and integrate it with our school's adaptive learning platform. \n",
    "The chatbot should be capable of doing content search in the system, doing quizzes, motivating, real-time interaction with students, resolution of misunderstandings, and delivering curriculum content in various formats. \n",
    "It also needs the ability to provide topic definitions, connect with a recommendation system, and engage in small talk for a more interactive experience.\n",
    "I want you to brainstorm requirements for me.''')\n",
    "print(chat_history.memory.chat_memory.messages[-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b580c8-24f0-4c13-90ad-e754b025d716",
   "metadata": {},
   "source": [
    "### Seq. #2 (shown on table VII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac76727b-6aac-4bec-92be-d1495949b83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a professional requirements engineer who helps users brainstorm more software requirements.\n",
      "\n",
      "\n",
      "Current conversation:\n",
      "Human: As a software engineer, I want to build an AI chatbot and integrate it with our school's adaptive learning platform. \n",
      "The chatbot should be capable of doing content search in the system, doing quizzes, motivating, real-time interaction with students, resolution of misunderstandings, and delivering curriculum content in various formats. \n",
      "It also needs the ability to provide topic definitions, connect with a recommendation system, and engage in small talk for a more interactive experience.\n",
      "I want you to brainstorm requirements for me.\n",
      "AI: 1 - The chatbot shall perform content search in the Learning Management System (LMS).\n",
      "2 - The chatbot shall carry out Quizzes.\n",
      "3 - The Chatbot shall Motivate Students.\n",
      "4 - The Chatbot shall have Real-Time Interaction With Students.\n",
      "5 - The Chatbot shall resolve Misunderstandings.\n",
      "6 - The Chatbot shall deliver Curriculum Content in Various Formats.\n",
      "7 - The Chatbot shall Provide Topic Definitions.\n",
      "8 - The Chatbot shall Connect To A Recommendation System.\n",
      "9 - The Chatbot shall Engage In Small Talk.\n",
      "user:\n",
      "user: Concerning requirement number 3, which you proposed in your previous message, tell me two methods I can employ to motivate students?\n",
      "assistant:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.15` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Sure! Here are two ways to motivate students:\n",
      "1 - Utilize gamification techniques such as points, badges, leaderboards, and progress bars.\n",
      "2 - Implement personalized study plans based on student performance and preferences.\n"
     ]
    }
   ],
   "source": [
    "llm_re_elicitor(chat_history, '''Concerning requirement number 3, which you proposed in your previous message, tell me two methods I can employ to motivate students?''')\n",
    "print(chat_history.memory.chat_memory.messages[-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6270deec-4b16-4f65-a65b-fdbb0d621500",
   "metadata": {},
   "source": [
    "### Seq. #3 (shown on table VII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3aca826-e178-454a-99ba-ac6fc3b4b1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a professional requirements engineer who helps users brainstorm more software requirements.\n",
      "\n",
      "\n",
      "Current conversation:\n",
      "Human: As a software engineer, I want to build an AI chatbot and integrate it with our school's adaptive learning platform. \n",
      "The chatbot should be capable of doing content search in the system, doing quizzes, motivating, real-time interaction with students, resolution of misunderstandings, and delivering curriculum content in various formats. \n",
      "It also needs the ability to provide topic definitions, connect with a recommendation system, and engage in small talk for a more interactive experience.\n",
      "I want you to brainstorm requirements for me.\n",
      "AI: 1 - The chatbot shall perform content search in the Learning Management System (LMS).\n",
      "2 - The chatbot shall carry out Quizzes.\n",
      "3 - The Chatbot shall Motivate Students.\n",
      "4 - The Chatbot shall have Real-Time Interaction With Students.\n",
      "5 - The Chatbot shall resolve Misunderstandings.\n",
      "6 - The Chatbot shall deliver Curriculum Content in Various Formats.\n",
      "7 - The Chatbot shall Provide Topic Definitions.\n",
      "8 - The Chatbot shall Connect To A Recommendation System.\n",
      "9 - The Chatbot shall Engage In Small Talk.\n",
      "user:\n",
      "Human: Concerning requirement number 3, which you proposed in your previous message, tell me two methods I can employ to motivate students?\n",
      "AI: Sure! Here are two ways to motivate students:\n",
      "1 - Utilize gamification techniques such as points, badges, leaderboards, and progress bars.\n",
      "2 - Implement personalized study plans based on student performance and preferences.\n",
      "user: Amazing! Can you turn your ways to motivating students into requirements?\n",
      "assistant:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.15` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Yes, here are two requirements:\n",
      "1 - The chatbot shall motivate students through the use of gamification techniques like points, badges, leaderboards, and progress bars.\n",
      "2 - The chatbot shall motivation by implementing personalized study plans based on student performance and preferences.\n"
     ]
    }
   ],
   "source": [
    "llm_re_elicitor(chat_history, \"Amazing! Can you turn your ways to motivating students into requirements?\")\n",
    "print(chat_history.memory.chat_memory.messages[-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b635d56-759d-4db8-bacc-d4ad9912cf8c",
   "metadata": {},
   "source": [
    "### Seq. #4 (shown on table VII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29115c58-fa2d-40c7-a6ff-c94e2623d5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a professional requirements engineer who helps users brainstorm more software requirements.\n",
      "\n",
      "\n",
      "Current conversation:\n",
      "Human: As a software engineer, I want to build an AI chatbot and integrate it with our school's adaptive learning platform. \n",
      "The chatbot should be capable of doing content search in the system, doing quizzes, motivating, real-time interaction with students, resolution of misunderstandings, and delivering curriculum content in various formats. \n",
      "It also needs the ability to provide topic definitions, connect with a recommendation system, and engage in small talk for a more interactive experience.\n",
      "I want you to brainstorm requirements for me.\n",
      "AI: 1 - The chatbot shall perform content search in the Learning Management System (LMS).\n",
      "2 - The chatbot shall carry out Quizzes.\n",
      "3 - The Chatbot shall Motivate Students.\n",
      "4 - The Chatbot shall have Real-Time Interaction With Students.\n",
      "5 - The Chatbot shall resolve Misunderstandings.\n",
      "6 - The Chatbot shall deliver Curriculum Content in Various Formats.\n",
      "7 - The Chatbot shall Provide Topic Definitions.\n",
      "8 - The Chatbot shall Connect To A Recommendation System.\n",
      "9 - The Chatbot shall Engage In Small Talk.\n",
      "user:\n",
      "Human: Concerning requirement number 3, which you proposed in your previous message, tell me two methods I can employ to motivate students?\n",
      "AI: Sure! Here are two ways to motivate students:\n",
      "1 - Utilize gamification techniques such as points, badges, leaderboards, and progress bars.\n",
      "2 - Implement personalized study plans based on student performance and preferences.\n",
      "Human: Amazing! Can you turn your ways to motivating students into requirements?\n",
      "AI: Yes, here are two requirements:\n",
      "1 - The chatbot shall motivate students through the use of gamification techniques like points, badges, leaderboards, and progress bars.\n",
      "2 - The chatbot shall motivation by implementing personalized study plans based on student performance and preferences.\n",
      "user: Give me three performance requirements for the chatbot.\n",
      "assistant:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.15` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1 - The chatbot shall achieve a 90% accuracy rate when responding to questions about academic topics.\n",
      "2 - The chatbot shall respond to simple requests in less than 1 second.\n",
      "3 - The chatbot shall handle complex queries in under 5 seconds 90% of the time.\n"
     ]
    }
   ],
   "source": [
    "llm_re_elicitor(chat_history, \"Give me three performance requirements for the chatbot.\")\n",
    "print(chat_history.memory.chat_memory.messages[-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd66630-b6d9-48a5-b67b-d9ad7ea87a87",
   "metadata": {},
   "source": [
    "### Seq. N, continue your own conversation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04fce2e1-39fa-49b5-8a9d-0d9660d5907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in between '''   ''' your text for continuation\n",
    "\n",
    "# llm_re_elicitor(chat_history, ''' ''')\n",
    "# print(chat_history.memory.chat_memory.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d13e30a-44ea-4f94-a182-a832b9f5d615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
