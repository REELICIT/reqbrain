{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c762a3-9091-4d53-9feb-ba40515d7604",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github//REELICIT/reqbrain_rep_package/blob/ee75058cf6ca3ff6fdf3cd6eeb1566ba61e95d24/inferencing_all_trained_models/all_models_used_with_langchain/mistralai-instruct_inferance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf07d98-16e3-48e0-9b4e-70de61d357b5",
   "metadata": {},
   "source": [
    "# ReqBrain (Mistralai 7b) Download & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49be96bd-de36-48cd-8ecd-4f6048a8b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "\n",
    "# if not installed, use !pip install <name of the API>\n",
    "# in case of error due to lacking API:\n",
    "# Python writes the name and command to install the lacking API in the very last lines of the error message.\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch import cuda, bfloat16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d091a8-da5a-45dd-9b8e-4170099f8743",
   "metadata": {},
   "source": [
    "# Resources Checkup\n",
    "\n",
    "- **Important Note:** A GPU with minimum of 32GB GPU memory is required to load the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d92655b-0267-4cf7-8f02-fccb17beaf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 1 Name: Tesla V100-SXM2-32GB\n",
      "GPU 1 Total Memory: 31.74 GB\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "\n",
    "    # Iterate over each GPU and print its name and memory information\n",
    "    for i in range(num_gpus):\n",
    "        gpu = torch.cuda.get_device_properties(i)\n",
    "        print(f\"GPU {i + 1} Name: {gpu.name}\")\n",
    "        print(f\"GPU {i + 1} Total Memory: {gpu.total_memory / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. A GPU with minimum of 32GB GPU memory is required to load the model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c660d47-9bd4-44f7-860f-74a5067bba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detecting GPU device to load model on it\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab6cc03-ae04-4ab5-838a-3833d5c81adf",
   "metadata": {},
   "source": [
    "# Downloading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b304279-e44b-41e4-9e9b-15aafc1572d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b207cae9c19456190144e829af0056f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_name = 'REELICIT/Mistral-7B-Instruct-v0.2-ReqBrain'\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, use_fast = False)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec08fb0b-6297-48a1-a47f-7d1d53a8c8d4",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abcdb1fb-447a-4033-81e1-2ff0bde8487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_token_ids = [\n",
    "    tokenizer.convert_tokens_to_ids(x) for x in [[\"INST\"], [tokenizer.eos_token], [\"end\", \"of\", \"list\"], [\"end\", \"_\", \"of\", \"_\"]]\n",
    "]\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "106fa892-abd9-49fa-9dd3-4a27457a9281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c50a943-e82d-441b-a3ff-5aaa443b06e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 11:47:30.704878: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-22 11:47:31.146198: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-22 11:47:31.146245: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-22 11:47:31.146281: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-22 11:47:31.170530: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-22 11:47:33.793277: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "pipe = transformers.pipeline(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    return_full_text = True, # Set it to True when combining with LangChain\n",
    "    task = 'text-generation',\n",
    "    device = device,\n",
    "    stopping_criteria = stopping_criteria,  \n",
    "    temperature = 0.1,\n",
    "    top_p = 0.15,  \n",
    "    top_k = 0,  \n",
    "    max_new_tokens = 200,  # the number of tokens the model must generate\n",
    "    repetition_penalty = 1.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfbd78c-3c22-4bba-a0f3-1bf39a91cd2f",
   "metadata": {},
   "source": [
    "# Inferance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eff4e48f-be4a-4d68-b586-1049de280188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.15` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The requirement shall use the correct sequence of symbols unfamiliar to an average user less than 2% of times.\n",
      "- The system shall identify incorrect input of CIFA58 token by beep sound and display message on LCD screen within 0.1 seconds after insertion.\n",
      "- The system shall recognize a valid CIFA58 token by making a long beep sound followed by displaying the corresponding fare on LCD screen within 0.3 seconds from identification.  [/INST\n"
     ]
    }
   ],
   "source": [
    "prompt_1 = \"I want two usability requirements for bus ticket machines.\" \n",
    "\n",
    "result = pipe(f\"<s>[INST] {prompt_1} [/INST]\")\n",
    "print(result[0]['generated_text'].split(\"[/INST]\")[-1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0e36ecd-15b0-4a35-93c5-4f03d7e6caf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The requirement shall use the correct sequence of symbols unfamiliar to an average user less than 2% of times.\n",
      "- The system shall identify incorrect input of CIFA58 token by beep sound and display message on LCD screen within 0.1 seconds after insertion.\n",
      "- The system shall recognize a valid CIFA58 token by making a long beep sound followed by displaying the corresponding fare on LCD screen within 0.3 seconds from identification.  [/INST\n"
     ]
    }
   ],
   "source": [
    "prompt_2 = \"Write me three security requirements for an ATM software?\"\n",
    "\n",
    "result = pipe(f\"<s>[INST] {prompt_1} [/INST]\")\n",
    "print(result[0]['generated_text'].split(\"[/INST]\")[-1])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1616bc7-9646-40c0-abc8-eb3c4921fef0",
   "metadata": {},
   "source": [
    "## Try it for your self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70ba22f2-ca87-4692-9d0e-a1195d8a6da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below lines to use a custom query\n",
    "\n",
    "# your_prompt = 'your prompt goes here!'\n",
    "# result = pipe(f\"<s>[INST] {your_prompt} [/INST]\")\n",
    "# print(result[0]['generated_text'].split(\"[/INST]\")[-1])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e9bc16-55ec-4287-a29b-f307c133826a",
   "metadata": {},
   "source": [
    "# Model Setup with Langchain for Chat-based RE Elicitation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f8de78e-4ee7-4bfe-9326-86fd73b7001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing langchain required APIs\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf456ecc-f836-4b60-9fc7-2ed87e9b8253",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = HuggingFacePipeline(pipeline = pipe)\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key = \"history\",\n",
    "    k = 5,     # Chat history memory, increase/decrease it depending on how far the model should remember!\n",
    "    return_only_outputs = True)\n",
    "\n",
    "chat_history = ConversationChain(\n",
    "    llm = chat_model,\n",
    "    memory = memory,\n",
    "    verbose = True)\n",
    "\n",
    "chat_history.prompt.template = \"\"\"\n",
    "You are a professional requirements engineer who helps users brainstorm more software requirements.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "User: {input}\n",
    "Assistant:  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcb7b90f-a431-412f-a812-6ee2e5cebc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_re_elicitor(chat_chain, query):\n",
    "    chat_chain.predict(input = query)\n",
    "    last_message = chat_chain.memory.chat_memory.messages[-1]\n",
    "    last_message_content = last_message.content.split('\\n\\n')[0].strip()\n",
    "    \n",
    "    # cleaning prtompt text\n",
    "    prompt_id = last_message_content.find('\\n### Human:')\n",
    "    if prompt_id != -1:\n",
    "        last_message_content = last_message_content[:prompt_id]\n",
    "\n",
    "    # cleaning model generated text\n",
    "    for stop_token in ['***', '###', 'User:']:\n",
    "        last_message_content = last_message_content.removesuffix(stop_token)\n",
    "\n",
    "    return last_message_content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0cde6af-a0a1-492e-b5e0-dbad84524bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.15` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a professional requirements engineer who helps users brainstorm more software requirements.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "User: As a software engineer, I want to build an AI chatbot and integrate it with our school's adaptive learning platform. \n",
      "The chatbot should be capable of doing content search in the system, doing quizzes, motivating, real-time interaction with students, resolution of misunderstandings, and delivering curriculum content in various formats. \n",
      "It also needs the ability to provide topic definitions, connect with a recommendation system, and engage in small talk for a more interactive experience.\n",
      "I want you to brainstorm requirements for me.\n",
      "Assistant:  \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "1) The chatbot shall function properly with no crashes under test conditions;\n",
      "2) The chatbot must enable users (teachers/admin/students) to access its functionalities seamlessly;\n",
      "3) The chatbot  QA (Quality Assurance) checks have revealed no bugs or issues;\n",
      "4) The chatbot interface(UI& UX Designing) is user-friendly and visually appealing;\n",
      "5) The chatbot will work effectively on Google Chrome, Microsoft Edge, Safari, and Firefox browsers;\n",
      "6) The chatbot  Question & Answer (Q&A) session with developers has been successful;\n",
      "7) The chatbot \n",
      "- Shall \n",
      "  - Get details about the student from the profile\n",
      "  - Understand the context of the question posed by the teacher\n",
      "  - Find the correct answer using external APIs\n",
      "  - Reply to the teacher with the answer\n",
      "  -\n"
     ]
    }
   ],
   "source": [
    "llm_re_elicitor(chat_history, '''As a software engineer, I want to build an AI chatbot and integrate it with our school's adaptive learning platform. \n",
    "The chatbot should be capable of doing content search in the system, doing quizzes, motivating, real-time interaction with students, resolution of misunderstandings, and delivering curriculum content in various formats. \n",
    "It also needs the ability to provide topic definitions, connect with a recommendation system, and engage in small talk for a more interactive experience.\n",
    "I want you to brainstorm requirements for me.''')\n",
    "print(chat_history.memory.chat_memory.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13463585-ed8c-40fb-9568-624aa98c5024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.15` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a professional requirements engineer who helps users brainstorm more software requirements.\n",
      "\n",
      "Current conversation:\n",
      "Human: As a software engineer, I want to build an AI chatbot and integrate it with our school's adaptive learning platform. \n",
      "The chatbot should be capable of doing content search in the system, doing quizzes, motivating, real-time interaction with students, resolution of misunderstandings, and delivering curriculum content in various formats. \n",
      "It also needs the ability to provide topic definitions, connect with a recommendation system, and engage in small talk for a more interactive experience.\n",
      "I want you to brainstorm requirements for me.\n",
      "AI: \n",
      "1) The chatbot shall function properly with no crashes under test conditions;\n",
      "2) The chatbot must enable users (teachers/admin/students) to access its functionalities seamlessly;\n",
      "3) The chatbot  QA (Quality Assurance) checks have revealed no bugs or issues;\n",
      "4) The chatbot interface(UI& UX Designing) is user-friendly and visually appealing;\n",
      "5) The chatbot will work effectively on Google Chrome, Microsoft Edge, Safari, and Firefox browsers;\n",
      "6) The chatbot  Question & Answer (Q&A) session with developers has been successful;\n",
      "7) The chatbot \n",
      "- Shall \n",
      "  - Get details about the student from the profile\n",
      "  - Understand the context of the question posed by the teacher\n",
      "  - Find the correct answer using external APIs\n",
      "  - Reply to the teacher with the answer\n",
      "  -\n",
      "User: 'Concerning requirement number 3, which you proposed in your previous message, tell me two methods I can employ to motivate students?\n",
      "Assistant:  \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "8) The Chatbot \n",
      "- Shall\n",
      "  - Provide definition of topics\n",
      "  - Connect with Recommendation System\n",
      "  - Engage in Small Talk\n",
      " 9) The Chatbot \n",
      "- Shall\n",
      "  - Search for content within the system\n",
      "  - Do Quizzes\n",
      "  - Have Real-time Interaction with Students\n",
      " 10) The Chatbot \n",
      "- Shall\n",
      "    - Deliver Curriculum Content in different formats \n",
      "    - Resolve Misunderstanding\n",
      "Based on your input, here are some additional requirements that could be derived:\n",
      "11) The Chatbot\n",
      "- Shall\n",
      "  - Work efficiently without lags\n",
      "  - Be compatible with the latest version of Windows OS\n",
      " 12) The Chatbot\n",
      "- Shall\n",
      "  - Inform the User if there's a network issue\n",
      " 13) The Chatbot\n",
      "- Shall\n",
      "  - Understand the\n"
     ]
    }
   ],
   "source": [
    "llm_re_elicitor(chat_history, ''''Concerning requirement number 3, which you proposed in your previous message, tell me two methods I can employ to motivate students?''')\n",
    "print(chat_history.memory.chat_memory.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e66d85cf-ec46-43c8-a470-435417a7299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your prtompt between ''' marks for continuation of the chat!\n",
    "\n",
    "# llm_re_elicitor(chat_history, '''' ''')\n",
    "# print(chat_history.memory.chat_memory.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a6e0ae-cfa2-4d76-a32a-2b2ff177c574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
